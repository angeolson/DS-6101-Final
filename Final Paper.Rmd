---
title: "DATS 6101 Music Genre Prediction"
author: "Ange Olson, Pavani Samal, Meghana Gantla, Kowshik Bezawada"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
library(ezids)
#install.packages("nnet")
library(nnet)
```

# Section I: Intro 

What defines a music genre? According to Franco Fabbri, it is "a set of musical events (real or possible) whose course is governed by a definite set of socially accepted rules." (Fabbri, 1982). These rules, however, can be difficult to pin down or put in so many words. Often, when people think of a music genre, they can hear it rather than describe it. The rule for determining the genre of a song may mirror the decision made in Jacobellis v. Ohio: "I know it when I hear it." Still, is it possible to determine characteristics of a genre more scientifically? Such a finding could help inform playlist curation, help listeners explore new music and find tunes they might like based on similarity to preferred genre, or could help artists better understand their distinct sound.

Spotify, a leading music streaming service, collects metadata on the songs on its platform. This data includes basic information on a song, such as the artist, album, date released, and genre, as well as more in depth audio features, described below:

* **Acousticness:** On a continuous scale of 0.0 to 1.0, how confident Spotify is that a song is acoustic, with 1.0 representing high confidence.
* **Danceability:** On a continuous scale of 0.0 to 1.0, how danceable a song is, where "danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity." 
* **Energy:** On a continuous scale of 0.0 to 1.0, "a perceptual measure of intensity and activity." Spotify gives the example of death metal as high energy, and a Bach prelude as having low energy. this score is determined by "dynamic range, perceived loudness, timbre, onset rate, and general entropy."
* **Instrumentalness:** On a continuous scale of 0.0 to 1.0, how confident Spotify is that a song contains no vocals. According to Spotify, Rap and spoken word are vocal, with any value above 0.5 a likely vocal track.
* **Liveness:** On a continuous scale of 0.0 to 1.0, how confident Spotify is that there is an audience in a recording (i.e. that the song was recorded live and/or at a concert). According to Spotify, "a value above 0.8 provides strong likelihood that the track is live."
* **Loudness:** On a continuous scale, this measures in decibels (dB) the loudness of a song averaged over its entire duration (Sptofy can provide different measurements for this feature for different parts of a song). According to Spotify, "loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude)." Values tend to exist between -60 and 0 db.
* **Speechiness:** In contrast to 'instrumentalness,' a continuous scale of 0.0 to 1.0 this measure describes the confidence that a given song (or track) is more "exclusively speech-like." As examples, Spotify notes that podcasts, audio books, or true spoken word tracks will score close to 1.0 (though anything over 0.66 likely falls in this category), an rap exists between 0.33 and 0.66 typically. Anything below 0.33 is what we would generally consider traditional music. 
* **Valence:** On a continuous scale of 0.0 to 1.0, this measure decribes the musical positiveness/happiness of a track. A score closer to 1.0 suggests the song is very happy, where a score closer to 0.0 suggests that the song is sad or angry. 

Spotify collects other useful data elements as well, including tempo (beats per measure); key, a categorical variable denoting the key of a song (A, A#, B, etc.) as an integer; and mode (major or minor) as a binary variable. 

With this data, we aim to see if we can predict music genre through modelling techniques including Logistic regression, KNN modeling, and categorical decision trees. The accuracy scores of these give a sense of how challenging it is to determine genre based on these audio features (and for which genres it is most challenging). The coefficients, or weight/importance given to each feature, can tell us which features relate most strongly to which genres and can help us quantify what it means to be in genre x or genre y. These findings serve to answer our SMART questions, defined in Section II.

In sum, we find our models that focused on predicting whether or not a song fell into a specific genre or not to be more accurate (e.g. is this song Rock, or anything else?), however in terms of interpreting our predictions, our multinomial logit model was the most useful because it allowed us to see 1) where we failed to predict the genre successfully and why and 2) where we predict successfully (or not), what other genres is a song most similar to. 

# Section II: SMART Questions and Model Selection 

We decided on three different SMART questions for our analysis:

1. What are key characteristics of each genre?
2. What genres of music are least distinguishable from each other?
3. Which genre(s) is/are the easiest to predict?

We aim to answer question one through our Exploratory Data Analysis (EDA). Identifying differences in the means of the various audio features we have chosen to study (listed in Section I) can tell us which genres are the happiest, the loudest, the most popular, etc. We can also determine this most clearly through our logit models. The coefficients on our individual binomial logit models can give us a good profile of each genre by informing what the most significant (and statistically significant) variables are specific to that genre. 

Question two can be answered most clearly through our multinomial logit regression model, particularly by looking at areas of high misclassification and by seeing which genres have the most similar model coefficients. 

Lastly, question three can be answered through a combination of all of our models. We look to see based on accuracy which models do poorly, and which perform well to determine which genres are the easiest to predict. 

# Section III: EDA

## Data Loading and Cleaning

```{r}
music = data.frame(read.csv("music_genre.csv"))
```

First, we look at how many genres there are and how many observations are in each genre. In the table below, we see that we are looking at `r nrow(music$music_genre)` genres, each with roughly 4,500 observations, making this a balanced dataset. We remove blank (" ") genres.

```{r, results = 'markup'}
library(dplyr)
genres <- music %>%
     group_by(music_genre) %>%
     count()
genres
```

All keys (major and minor) are represented in the dataset, and the distribution of songs in each key across all genres does appear to be statistically significantly different at  the $\alpha$ = 0.05 level.

```{r, results = 'markup'}
Cont_Table <- table(music$music_genre, music$key)
xkabledply(Cont_Table, title="Contingency Table for Genre and Key")
chitest = chisq.test(Cont_Table)
chitest
```

```{r}
# drop those blank genre values 
music = music[!(music$music_genre == ""),]

# convert tempo to numeric
music$tempo <- as.numeric(music$tempo)

# created some na values...let's see what

tempoNA <- music %>%
     group_by(is.na(music$tempo)) %>%
     count()
tempoNA
# not enough to cause concern 
```
```{r}

# drop those blank genre values 
music = music[!(music$music_genre == ""),]

# key, mode, genre as factor
music$key <- factor(music$key)
music$mode <- factor(music$mode)
music$music_genre <- factor(music$music_genre)

# tempo as numeric
music$tempo <- as.numeric(music$tempo)

# drop na values
music = music[!(is.na(music$tempo)),]
```

Below, we test for statistically significant differences in our continuous variables using ANOVA tests and find that all show differences in means.

```{r, results = 'markup'}
# simplify dataset 
musicNum <- music[, c(4:9, 11:12, 14:15, 17, 18)]

# run tests
library(tidyverse)
anova_results <- purrr::map(musicNum[,1:11], ~summary(aov(.x ~ musicNum$music_genre)))
anova_results
```


# Section IV: Logit Modeling

We developed two types of Logit models to predict genre; multinomial logit regression and binomial logit regression. The multinomial model allows us to see what the second, third, etc. most likely genres are to get a better sense of where the model might be mis-classifying and why. Given the similarities between some genres, we expect that there would be some genres where this type of model might predict poorly. A binomial model won't allow us to see what the likeliest genres for a song might be, but because we can tailor each model to classify a particular genre, we may obtain better accuracy.

## Feature Selection

Before developing our model, we look to see which variables are correlated with each other to ensure we are not including variables that are too correlated. As shown below, we don’t want to include both `energy` and `acousticness`, or both `acousticness` and `loudness`, or `energy` and `loudness`. Since `energy` is correlated with `loudness` and `acousticness`, we leave it in the models to capture both those effects. 

```{r, results='markup'}
loadPkg("corrplot")
music_cor <- music[, c(4:9, 11:12, 14:15, 17)]
corrplot(cor(music_cor, method = "spearman"), method = "circle", type = "upper")
```

```{r}
# prepare training and test data
set.seed(100)
trainingRows <- sample(1:nrow(music), 0.7*nrow(music))
training <- music[trainingRows, ]
test <- music[-trainingRows, ]
```

## Model I 

Our first multinomial Logit model includes the following variables:

* `popularity`
* `danceability`
* `duration_ms`
* `energy`
* `instrumentalness`
* `key`
* `liveness`
* `mode`
* `speechiness`
* `tempo`
* `valence`

All variables are statistically significant according to 2-tailed Wald tests at  the $\alpha$ = 0.05 level.

```{r, results = 'markup'}
# build model
multinomModel <- multinom(music_genre ~ popularity + danceability + duration_ms + energy + instrumentalness + key + liveness + mode + speechiness + tempo + valence, data=training) # multinom Model

model1 <- summary(multinomModel) 
model1

z <- model1$coefficients/model1$standard.errors
# 2-tailed Wald z tests to test significance of coefficients
```

Below are the p-values for the coefficients of the model based on Wald 2-sample tests. All p values are below 0.05. 

```{r}
p <- (1 - pnorm(abs(z), 0, 1)) * 2
xkabledply(p, title = "P-Values for Coefficients in Model 1")
```

```{r}
data1 <- list()
data2 <-list()
#cols <- c("Alt", "Alt-Rock", "Anime", "Blues", "Blues-Jazz", "Class", "Country", "Elec", "Hip-Hop", "HH-Rap", "Jazz", "Jazz-Blues", "Rap", "Rap-HH", "Rock", "Rock-Alt")
```

### Model I Performance 

```{r, results = 'markup'}
# see how model1 does

predicted_class <- predict (multinomModel, test)
table <- table(predicted_class, test$music_genre)
xkabledply(table, title = "Model 1 Confusion Matrix")


# Alt: .38
data1 <- c(data1, table[1,1]/sum(table[1,]))

# Alt or Rock? .51
# data1 <- c(data1, (table[1,1] + table[1,10])/sum(table[1,]))

# Anime: .60
data1 <- c(data1, table[2,2]/sum(table[2,]))

# Blues: .49
data1 <- c(data1, table[3,3]/sum(table[3,]))

# Blues or Jazz? .61
# data1 <- c(data1, (table[3,3] + table[3,8])/sum(table[3,]))

# Class: .70
data1 <- c(data1, table[4,4]/sum(table[4,]))

# Country: .44
data1 <- c(data1, table[5,5]/sum(table[5,]))

# Elec: .57
data1 <- c(data1, table[6,6]/sum(table[6,]))

# Hip-Hop: .44
data1 <- c(data1, table[7,7]/sum(table[7,]))

# Hip Hop or rap? .79
# data1 <- c(data1, (table[7,7] + table[7,9])/sum(table[7,]))

# Jazz: .50
data1 <- c(data1, table[8,8]/sum(table[8,]))

# Jazz or Blues? .62
# data1 <- c(data1, (table[8,8] + table[8,3])/sum(table[8,]))

# Rap: .43
data1 <- c(data1, table[9,9] /sum(table[9,]))

# Rap or Hip Hop? .88
# data1 <- c(data1, (table[9,9] + table[9,7])/sum(table[9,]))

# Rock: .53
data1 <- c(data1, table[10,10] /sum(table[10,]))

# Rock or Alt? .65
# data1 <- c(data1, (table[10,10] + table[10,1])/sum(table[10,]))
```



Now, we look to see how the model performs. The confusion matrix is listed below, and the total accuracy is `r mean(as.character(predicted_class) == as.character(test$music_genre))`. So, on average across all genres, the model predicts the correct genre roughly 68% of the time. The most challenging genres to predict were Alternative (accuracy: `r c(table[1,1]/sum(table[1,]))`) and Rap (accuracy: `r (table[9,9] /sum(table[9,]))`. However, these genres are similar to Rock and Hip-Hop respectively. If we include those genres as "correct," accuracy jumps for Alternative to `r (table[1,1] + table[1,10])/sum(table[1,])` and Rap to `r `(table[9,9] + table[9,7])/sum(table[9,])`. 

If we predicted genres at random, we could expect to be right 10% of time time, so this accuracy rate is an improvement over a null model. Next, we look to see if we can simplify the model without sacrificing accuracy. 


Accuracy rates for each genre are listed below:

```{r, results = 'markup'}
gens <- c("Alternative", "Anime", "Blues", "Classical", "Country", "Electronic", "Hip-Hop", "Jazz", "Rap", "Rock")
model1_data = data.frame(unlist(data1), row.names = gens)
names(model1_data) = "Accuracy"
xkabledply(model1_data, title = "Accuracy")
```

Alternative and Rock, Blues and Jazz, and Hip Hop and Rap are similar to each other. If we consider these genres with expected overlap and similarities as "successes" if predicted, accuracy rates change to the following: 

* Predicting Alternative (accept Alternative or Rock): `r (table[1,1] + table[1,10])/sum(table[1,])`
* Predicting Rock (accept Alternative or Rock): `r (table[10,10] + table[10,1])/sum(table[10,])`
* Predicting Jazz (accept Jazz or Blues): `r (table[8,8] + table[8,3])/sum(table[8,])`
* Predicting Blues (accept Jazz or Blues): `r (table[3,3] + table[3,8])/sum(table[3,])`
* Predicting Hip Hop (accept Hip Hop or Rap): `r (table[7,7] + table[7,9])/sum(table[7,])`
* Predicting Rap (accept Hip Hop or Rap): `r (table[9,9] + table[9,7])/sum(table[9,])`

## Model II

This next model includes the following variables (does not include `key` to try and simplify the model):

* `popularity`
* `danceability`
* `duration_ms`
* `energy`
* `instrumentalness`
* `liveness`
* `mode`
* `speechiness`
* `tempo`
* `valence`

All variables are statistically significant according to 2-tailed Wald tests at the $\alpha$ = 0.05 level.

```{r}
# build model2
multinomModel2 <- multinom(music_genre ~ popularity + danceability + duration_ms + energy + instrumentalness + liveness + mode + speechiness + tempo + valence, data=training) # multinom Model

model2 <- summary(multinomModel2) 
model2

z <- model2$coefficients/model2$standard.errors
# 2-tailed Wald z tests to test significance of coefficients
p <- (1 - pnorm(abs(z), 0, 1)) * 2
print("Wald Test P-Values: ")
p
```

### Model II Performance 




```{r, results = 'markup'}
# see how model2 does

predicted_class2 <- predict (multinomModel2, test)

# All
table <- table(predicted_class2, test$music_genre)
# mean(as.character(predicted_class2) != as.character(test$music_genre))
xkabledply(table, title = "Model 2 Confusion Matrix")

# Alt: .37
data2 <- c(data2, table[1,1]/sum(table[1,]))

# Alt or Rock? .52
# data2 <- c(data2, (table[1,1] + table[1,10])/sum(table[1,]))

# Anime: .61
data2 <- c(data2, table[2,2]/sum(table[2,]))

# Blues: .49
data2 <- c(data2, table[3,3]/sum(table[3,]))

# Blues or Jazz? .62
# data2 <- c(data2, (table[3,3] + table[3,8])/sum(table[3,]))

# Class: .71
data2 <- c(data2, table[4,4]/sum(table[4,]))

# Country: .43
data2 <- c(data2, table[5,5]/sum(table[5,]))

# Elec: .56
data2 <- c(data2, table[6,6]/sum(table[6,]))

# Hip-Hop: .46
data2 <- c(data2, table[7,7]/sum(table[7,]))

# Hip Hop or rap? .79
# data2 <- c(data2, (table[7,7] + table[7,9])/sum(table[7,]))

# Jazz: .52
data2 <- c(data2, table[8,8]/sum(table[8,]))

# Jazz or Blues? .63
# data2 <- c(data2, (table[8,8] + table[8,3])/sum(table[8,]))

# Rap: .42
data2 <- c(data2, table[9,9] /sum(table[9,]))

# Rap or Hip Hop? .87
# data2 <- c(data2, (table[9,9] + table[9,7])/sum(table[9,]))

# Rock: .53
data2 <- c(data2, table[10,10] /sum(table[10,]))

# Rock or Alt? .66
# data2 <- c(data2, (table[10,10] + table[10,1])/sum(table[10,]))
```

Again, we look to see how the model performs. The confusion matrix is listed above, and the total accuracy is `r mean(as.character(test$predicted_class2) == as.character(test$music_genre))`. So, on average across all genres, the model predicts the correct genre roughly 68% of the time, nearly the same as the previous model. Overall, all accuracy rates for all genres are roughly the same, and removing the dummy variable `key` has simplified the model without sacrificing accuracy. Still, accuracy for some genres (notably Alternative) could be improved, and we will look to create binomial logit models.

In terms of genre characteristics (SMART Question 1), both multinomial models tell us that compared to Alternative music:

* Hip Hop, Rap, and Rock are the most popular 
* Country, Electronic, Hip Hop, Jazz, and Rap are the most danceable
* Country and Anime songs are shorter
* Electronic and Anime songs have higher energy
* Electronic, Hip Hop, and Rap songs are not very happy 
* Jazz, Blues, and Classical songs tend to be slower 


## Individual Logit Models

To prep the data for individual models, we create subsets of the data that are balanced with an even mix of the genre in question we are looking to predict and a random sampling of all other genres. The Logit regression model summaries for each genre are presented below. Overall, we see most variables are statistically significant at the $\alpha$ = 0.05 level. 

Each model also tells us a bit more about each genre as compared to the average characteristics of all other genres. Not that 'strongest predictors' are determined by the magnitude of the coeffiicent of the variable. Because our variables of comparison are measured on relatively equal scales (0 to 1, continuous), we consider these apt comparisons. 

* Alternative: more popular, energetic, and likely to be in a minor key. *Strongest Predictors*: high energy, not danceable, not instrumental, not speechy
* Anime: more energetic and faster. *Strongest Predictors*: not danceable, energetic, not live, not speechy
* Blues: more likely to contain a wider variety of keys (including those less common to other genres), more likely to be in a minor key. *Strongest Predictors*: not danceable, not instrumental, not speechy, happy.
* Classical: less popular. *Strongest Predictors*: not danceable, energetic, instrumental.
* Country: Danceable and happy. *Strongest Predictors*: Not instrumental, not in minor keys. 
* Electronic: *Strongest Predictors/Characteristics*: Very danceable, very instrumental, very energetic, not happy.
* Hip Hop: More popular, more likley to be in a minor key. *Strongest Predictors*: Very danceable, not instrumental, very speechy, not happy.
* Jazz: Similar to the Blues, more likely to contain a wider variety of keys (including those less common to other genres) and also more likely to be in a minor key. *Strongest Predictors*: danceable, not energetic, instrumental, not speechy, happy.
* Rap: More popular, energetic. *Strongest Predictors*: danceable, not instrumental, speecy, not happy.
* Rock: More popular, less likley to be in minor keys. *Strongest Predictors*: not danceable, energetic, happy.  


```{r, results = 'markup'}
# Logit models by genre

# make list of genres 
gens <- c("Alternative", "Anime", "Blues", "Classical", "Country", "Electronic", "Hip-Hop", "Jazz", "Rap", "Rock")

# set seed
set.seed(100)

# create list 
data3 = list()

for (genre in gens) {
# create balanced dataset
  music$selgen <- ifelse(music$music_genre == genre, 1, 0)
  selectGenre <- music[music$selgen == 1, ]
  otherGenre <- music[music$selgen == 0, ]
  otherGenre_sample <- sample(1:nrow(otherGenre), nrow(selectGenre))
  otherGenre <- otherGenre[otherGenre_sample, ]
  modelData <- data.frame(rbind(selectGenre, otherGenre))

# split into test/train
  trainingRows <- sample(1:nrow(modelData), 0.7*nrow(modelData))
  training <- modelData[trainingRows, ]
  test <- modelData[-trainingRows, ]

# model
  Model <- glm(selgen ~ popularity + danceability + duration_ms + energy + instrumentalness + key + liveness + mode + speechiness + tempo + valence, data = modelData, family = "binomial")

  print(genre)
  print(summary(Model))
  music$predselgen <- round(predict(Model, newdata = music, type = "response"))
  table = table(music$selgen, music$predselgen)
  accuracy = (table[1,1] + table[2,2]) / nrow(music)
  data3 = c(data3, accuracy)
}

```

Below is a table comparing the accuracy of all the different logit models. The baseline accuracy score for the individual models is 0.5 (1 in 2 chance of randomly guessing correctly), as compared to 0.1 for the multinomial models. Depending on the application, it may be more useful to choose one model over another. While the accuracy scores are higher for each genre as a whole when individual models are used, information on what the next highest predicted genre would be is lost. When dealing with songs that are in cusp genres (like Alternative) or are otherwise different than their typical genre, individual models may be less useful than the multinomial model. 

```{r, results = 'markup'}
# make results of two model accuracies by genre into a df

df = data.frame(unlist(data1), unlist(data2), unlist(data3), row.names = gens)
names(df) = c("Model 1", "Model 2", "Genre Spec. Model")
xkabledply(df, title = "Model Accuracy Comparisons")
```

Now, how do predictions work? The most interesting predictions to look at are from the multinomial model, which tells us (if incorrect) what the predicted genre actually is.

In this sample, there are a few notable mistakes; the Anime songs are both classified as classical, the electronic song is classified as jazz (though, the repeated chords do have a jazz sound to them), and the operatic classic "La Donna è Mobile" was classified as country. "Under the Pressure" was classified as rap, though in a previous iteration (with a different random sample not shown) this song was correctly classified as rock. 

```{r, results = 'markup'}
test$predicted <- predicted_class2 <- predict (multinomModel2, test)
set.seed(369)
samples <- sample_n(test[, c(1, 2, 3, 18, 21)], 10)
xkabledply(samples, title = "Sample Predictions")
```
Interestingly, classical wasn't even the second closest match for "La Donna è Mobile," but country. 

```{r, results = 'markup'}
predict (multinomModel2, test[ test$instance_id == '38780', ], type = 'probs')
```


```{r}
# make list of genres 
gens <- "Classical"

# set seed
set.seed(100)

# create list 
data3 = list()

for (genre in gens) {
# create balanced dataset
  music$selgen <- ifelse(music$music_genre == genre, 1, 0)
  selectGenre <- music[music$selgen == 1, ]
  otherGenre <- music[music$selgen == 0, ]
  otherGenre_sample <- sample(1:nrow(otherGenre), nrow(selectGenre))
  otherGenre <- otherGenre[otherGenre_sample, ]
  modelData <- data.frame(rbind(selectGenre, otherGenre))

# split into test/train
  trainingRows <- sample(1:nrow(modelData), 0.7*nrow(modelData))
  training <- modelData[trainingRows, ]
  test <- modelData[-trainingRows, ]

# model
  Model_Class <- glm(selgen ~ popularity + danceability + duration_ms + energy + instrumentalness + key + liveness + mode + speechiness + tempo + valence, data = modelData, family = "binomial")

  music$predselgen <- round(predict(Model_Class, newdata = music, type = "response"))
  table = table(music$selgen, music$predselgen)
  accuracy = (table[1,1] + table[2,2]) / nrow(music)
  data3 = c(data3, accuracy)
}
```

```{r}
predict(Model_Class, test[ test$instance_id == '38780', ], type = 'response')
```

However, when using a model calibrated to predict classical or not, "La Donna è Mobile," is predicted to be classical--but barely, at a probability of `r predict (Model_Class, test[ test$instance_id == '38780', ], type = 'response')`. 

# Section V: KNN

# Section VI: Decision Tree

# Section VII: Conclusion

## Overall Results 

In sum, we found that the best predictors of a specific musical genre in terms of accuracy alone were our genre-specific models. Our binomial logit models and KNN models performed nearly identically for the selected genres, and our decision tree models provided some slight improvements over the logit and KNN models for most genres. Notably, the decision tree models for Alternative music and Jazz provide substantial improvement. 

However, while accuracy with these models is improved, they do not tell us much about why misclassification is occurring or where the predicted next best fits for a model are. Depending on the application of the models, a multinomial model may be a better fit. For example, if the goal is to use music features to automatically create playlists for each singular genre, our binary choice models are likely the better solution. However, for an artist curious to see where their style of music falls on more of spectrum, or an individual looking for music in a genre that's not what they typically listen too, but similar to their preferred genre, the multinomial model may be a better fit, even if accuracy is diminished.

## Lessons Learned and Model Improvement 

A next step to improve our models with the data we have readily available could be text analysis of artist names and/or song titles. Anecdotally, we can look at the subsample of our predicted data to see how text analysis would improve our model. For example, "追憶 - ゆきわりそう" is in Japanese; we may (or may not) want to associate songs with titles in Japanese more heavily with the Anime genre. Additionally, many classical songs contain numbers in the title (Act 3, Sonata No. 4). Looking at the title, one could guess that "La Donna é Mobile" would be classical rather than country. 

We could also consider adding interaction variables to adjust the weight given to certain factors. For example, "Under the Pressure" may be at risk of misclassification as rap because the words of the song are primarily spoken. However, it is also highly instrumental, which rap is not. By interacting these two variables, perhaps we would give less weight to speechiness (a strong predictor of rap) when the track is instrumental ('not instrumental' being a strong predictor of rap) and this would lead to tracks like this one not being misclassified. This could be done for other genres with a strong positive predictor and strong negative predictor. 

```{r, results = 'markup'}
xkabledply(samples, title = "Sample Predictions")
```
