---
title: "DATS 6101 Music Genre Prediction"
author: "Ange Olson, Pavani Samala, Meghana Gantla, Kowshik Bezawada"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
library(ezids)
#install.packages("nnet")
library(nnet)
```

# Section I: Intro 

What defines a music genre? According to Franco Fabbri, it is "a set of musical events (real or possible) whose course is governed by a definite set of socially accepted rules." (Fabbri, 1981). These rules, however, can be difficult to pin down or put in so many words. Often, when people think of a music genre, they can hear it rather than describe it. The rule for determining the genre of a song may mirror the decision made in Jacobellis v. Ohio: "I know it when I hear it." Still, is it possible to determine characteristics of a genre more scientifically? Such a finding could help inform playlist curation, help listeners explore new music and find tunes they might like based on similarity to preferred genre, or could help artists better understand their distinct sound.

Spotify, a leading music streaming service, collects metadata on the songs on its platform. This data includes basic information on a song, such as the artist, album, date released, and genre, as well as more in depth audio features, described below (Spotify, 2022):

* **Acousticness:** On a continuous scale of 0.0 to 1.0, how confident Spotify is that a song is acoustic, with 1.0 representing high confidence.
* **Danceability:** On a continuous scale of 0.0 to 1.0, how danceable a song is, where "danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity." 
* **Energy:** On a continuous scale of 0.0 to 1.0, "a perceptual measure of intensity and activity." Spotify gives the example of death metal as high energy, and a Bach prelude as having low energy. this score is determined by "dynamic range, perceived loudness, timbre, onset rate, and general entropy."
* **Instrumentalness:** On a continuous scale of 0.0 to 1.0, how confident Spotify is that a song contains no vocals. According to Spotify, Rap and spoken word are vocal, with any value above 0.5 a likely vocal track.
* **Liveness:** On a continuous scale of 0.0 to 1.0, how confident Spotify is that there is an audience in a recording (i.e. that the song was recorded live and/or at a concert). According to Spotify, "a value above 0.8 provides strong likelihood that the track is live."
* **Loudness:** On a continuous scale, this measures in decibels (dB) the loudness of a song averaged over its entire duration (Sptofy can provide different measurements for this feature for different parts of a song). According to Spotify, "loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude)." Values tend to exist between -60 and 0 db.
* **Speechiness:** In contrast to 'instrumentalness,' a continuous scale of 0.0 to 1.0 this measure describes the confidence that a given song (or track) is more "exclusively speech-like." As examples, Spotify notes that podcasts, audio books, or true spoken word tracks will score close to 1.0 (though anything over 0.66 likely falls in this category), an rap exists between 0.33 and 0.66 typically. Anything below 0.33 is what we would generally consider traditional music. 
* **Valence:** On a continuous scale of 0.0 to 1.0, this measure decribes the musical positiveness/happiness of a track. A score closer to 1.0 suggests the song is very happy, where a score closer to 0.0 suggests that the song is sad or angry. 

Spotify collects other useful data elements as well, including tempo (beats per measure); key, a categorical variable denoting the key of a song (A, A#, B, etc.) as an integer; and mode (major or minor) as a binary variable. 

With this data, we aim to see if we can predict music genre through modelling techniques including Logistic regression, KNN modeling, and categorical decision trees. The accuracy scores of these give a sense of how challenging it is to determine genre based on these audio features (and for which genres it is most challenging). The coefficients, or weight/importance given to each feature, can tell us which features relate most strongly to which genres and can help us quantify what it means to be in genre x or genre y. These findings serve to answer our SMART questions, defined in Section II.

In sum, we find our models that focused on predicting whether or not a song fell into a specific genre or not to be more accurate (e.g. is this song Rock, or anything else?), however in terms of interpreting our predictions, our multinomial logit model was the most useful because it allowed us to see 1) where we failed to predict the genre successfully and why and 2) where we predict successfully (or not), what other genres is a song most similar to. 

# Section II: SMART Questions and Model Selection 

We decided on three different SMART questions for our analysis:

1. What are key characteristics of each genre?
2. What genres of music are least distinguishable from each other?
3. Which genre(s) is/are the easiest to predict?

We aim to answer question one through our Exploratory Data Analysis (EDA). Identifying differences in the means of the various audio features we have chosen to study (listed in Section I) can tell us which genres are the happiest, the loudest, the most popular, etc. We can also determine this most clearly through our logit models. The coefficients on our individual binomial logit models can give us a good profile of each genre by informing what the most significant (and statistically significant) variables are specific to that genre. 

Question two can be answered most clearly through our multinomial logit regression model, particularly by looking at areas of high misclassification and by seeing which genres have the most similar model coefficients. 

Lastly, question three can be answered through a combination of all of our models. We look to see based on accuracy which models do poorly, and which perform well to determine which genres are the easiest to predict. 

# Section III: EDA

## Data Loading

```{r,results='markup'}
music = data.frame(read.csv("music_genre.csv"))
head(music)
```

Our initial data set has 50005 observations with 18 features. However, for this research we will not be including `instance_id`, `artist_name`, `track_name`, and `obtained date`. Before making any adjustments, our data set is described below, 

```{r, results='markup'}
str(music)
```

```{r}
# changing vectors
music$key <- factor(music$key)
music$mode <- factor(music$mode)
music$music_genre <- factor(music$music_genre)
music$tempo <- as.numeric(music$tempo)
```

We changed `key`, `mode`, and `music_genre` from character variable to factorial variable. We also changed `tempo` from character variable to numeric variable. After changing vectors, below is the summary of our data set. 

```{r, results='markup'}
xkablesummary(music)
```

```{r, results='hold'}
# checking and removing null values
print(colSums(is.na(music)))
music = na.omit(music)
```
Processing the data to remove the rows containing Null values, 4985 rows were removed. We now have `r nrow(music)` observations in our data set. Moving forward, we derived secondary data set with only the relevant numerical features for correlations.
 
```{r, results='markup'}
#seperating numeric values
numcolumns <- unlist(lapply(music,is.numeric))
music_cor <- music[,numcolumns]
music_cor <- music_cor[,-c(1)]

xkablesummary(music_cor)
```
looking at the general summary of this data set, we have 11 numeric variables that are relevant to this research. `acousticness`, `danceability`, `energy`, `instrumentalness`, `liveness`, `speechiness`, and `valence` have a range of 0 - 1, `popularity` ranges from 0 - 99, `duration_ms` ranges from -1 - 4497994, `loudness` ranges from -47 to 4, and `tempo` has a range of 34 - 221. Moving on We will look into the distribution of these features to understand them better.

## Histograms
```{r, results='hold'}
# Histograms for all numeric columns
loadPkg("corrplot")
par(mfrow=c(2,3))
for (i in 1:ncol(music_cor[,1: ncol(music_cor)]))
{  
hist(music_cor[, i], breaks = 20, main = names(music_cor[i]), col = COL2('PuOr', 20), xlab = "")
}
```
Looking at the distributions, we can observe that the data is not normally distributed. `duration_ms`, `liveness` and `speechiness` are displaying clear right-skew and `loudness` is left-skewed.

## qqplots
```{r, results='hold'}
#qqplots for all numeric columns
par(mfrow=c(2,3))
for (i in 1:ncol(music_cor[,1: ncol(music_cor)]))
{  
qqnorm(music_cor[, i], main = names(music_cor[i]), pch = 18, col = "purple4")
   qqline(music_cor[, i], col = "orange")
}
```
Further, we observe clear presence of outliers in the `duration_ms` column and `popularity`, `loudness`, `liveness`, `speechiness`, `tempo` and `instrumentalness` are showing abnormal distribution.


## Removing outliers
```{r, results='hide'}
music2 <- data.frame(music)

# changing vectors
music2$key <- factor(music2$key)
music2$mode <- factor(music2$mode)
music2$music_genre <- factor(music2$music_genre)
music2$tempo <- as.numeric(music2$tempo)

# Removing outliers
music2 = outlierKD2(music2, popularity, rm = TRUE, histogram = FALSE)
music2 = outlierKD2(music2, danceability, rm = TRUE, histogram = FALSE)
music2 = outlierKD2(music2, loudness, rm = TRUE, histogram = FALSE)
music2 = outlierKD2(music2, liveness, rm = TRUE, histogram = FALSE)
music2 = outlierKD2(music2, duration_ms, rm = TRUE, histogram = FALSE)
music2 = outlierKD2(music2, speechiness, rm = TRUE, histogram = FALSE)
music2 = outlierKD2(music2, tempo, rm = TRUE, histogram = FALSE)

# Seperating numeric values
numcolumns2 <- unlist(lapply(music2,is.numeric))
music_cor2 <- music2[,numcolumns2]
music_cor2 <- music_cor2[,-c(1)]
```

Removing the outliers by changing them to NA using the outliersKD2 function has affected 16210 observations out of 45020 observations. We have derived another data set (music2) for this process. The new dataset is described below, 

```{r, results='hold'}
xkabledply(head(music2), title = "Dataframe without outliers")
xkablesummary(music_cor2)
```
Looking at the general summary now, we can see that the minimum value of `duration_ms` has changed to 34040 from -1 and maximum has changed to 409507 from 4497994. `loudeness` has a range or -19 - 2, `tempo` has a range of 34 - 209 and `popularity` has a range of 1 - 89. 

## Histogram (without outliers)
```{r, results='markup'}
# Histograms for all numeric columns
par(mfrow=c(2,3))
for (i in 1:ncol(music_cor2[,1: ncol(music_cor2)]))
{  
hist(music_cor2[, i], breaks = 20, main = names(music_cor2[i]), col = COL2('PuOr', 20), xlab = "")
}
```

## qqplot (without outliers)
```{r, results='markup'}
# qqplots for all numerical columns
par(mfrow=c(2,3))
for (i in 1:ncol(music_cor2[,1: ncol(music_cor2)]))
{  
qqnorm(music_cor2[, i], main = names(music_cor2[i]), pch = 18, col = "purple4")
   qqline(music_cor2[, i], col = "red")
}

```

Looking at the distributions again, we can observe that most features now show symmetric/normal distribution. We have included both histograms and QQ plots again to show the changes. However, going into the analysis we have decided to keep these outliers as we do not want to completely exclude observations like audio books and meditation music, which generally have longer durations, from our research. 

## Genres

Here, we look at how many genres there are and how many observations are in each genre. In the table below, we see that we are looking at `r nrow(music$music_genre)` genres, each with roughly 4,500 observations, making this a balanced dataset. We remove blank (" ") genres.

```{r, results='hide'}
unique(music$music_genre)
```

```{r, results = 'markup'}
library(dplyr)
g <- music %>%
     group_by(music_genre) %>%
     count()
g
```


All keys (major and minor) are represented in the dataset, and the distribution of songs in each key across all genres does appear to be statistically significantly different at  the $\alpha$ = 0.05 level.

```{r, results='markup'}
library(dplyr)
genres = music %>% group_by(music_genre)  %>%
                    summarise(popularity = mean(popularity),
                              acousticness = mean(acousticness),
                              danceability = mean(danceability),
                              duration_ms = mean(duration_ms),
                              energy = mean(energy),
                              instrumentalness = mean(instrumentalness),
                              liveness = mean(liveness),
                              loudness = mean(loudness),
                              speechiness = mean(speechiness),
                              tempo = mean(tempo),
                              valence = mean(valence))
 
xkabledply(genres, title = "Averages for each genre")
```

We made a table with music genres as the rows, features as the columns displaying the average values of each characteristic in each music genre. `popularity` for Rap music tops the list with an average of 60.54 and Anime is at the bottom with an average of 24.79. Coming to `acousticness` feature, Classical music tops the list with average = 0.87 and Rap is at the last position with average of 0.16. `danceability`, Hip-Hop music has higher average at 0.713 and Classical has the least at an average of 0.306. `duration_ms`, Classical genre tops the list with an average of 278ms and Country is at last position with average of 195ms. When it comes to `energy`, the Electronic genre places first with an average of 0.73, while Classical places last with an average of 0.17. For `instrumentalness`, Classical music has the highest average at 0.60, which makes sense as we all know many instruments are used in the making of classical music, while hip-hop has the lowest average of 0.01. The Blues genre ranks first for `liveness` with an average of 0.23, contradictory to Classical which has an average of 0.16. In terms of the `loudness` feature, the Alternative genre ranks first with an average of -6.54, while Classical ranks bottom with an average of -21.60. For `speechiness` Hip-Hop genre has the highest average at 0.20, which was surprising as we expected Rap music to be on the top. `tempo` has it's highest average of 126 in Anime genre and lowest in the Classical at 104. Finally, `valence` has highest average of 0.57 for the Blues genre and Lowest average of 0.21 for Classical genre. 

To answer our first SMART question, we can look at the bar plots below to understand how each feature changes with genre
```{r, results='hold'}
#install.packages("patchwork")
library(patchwork)
p1 <- ggplot(genres, aes(music_genre, popularity, fill = music_genre)) +    
  geom_bar(stat = "identity") +
  labs(title='Average popularity', x = "Genre") +
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p2 <- ggplot(genres, aes(music_genre, acousticness, fill = music_genre)) +    
  geom_bar(stat = "identity") +
   labs(title='Average acoustiness', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p3 <- ggplot(genres, aes(music_genre, danceability, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average danceability', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p4 <- ggplot(genres, aes(music_genre, duration_ms, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average duration', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p5 <- ggplot(genres, aes(music_genre, energy, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average energy', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p6 <- ggplot(genres, aes(music_genre, instrumentalness, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average instrumentalness', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p7 <- ggplot(genres, aes(music_genre, liveness, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average liveness', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p8 <- ggplot(genres, aes(music_genre, loudness, fill = music_genre)) +     
  geom_bar(stat = "identity") +  scale_y_reverse() +
   labs(title='Average loudness', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p9 <- ggplot(genres, aes(music_genre, speechiness, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average speechiness', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p10 <- ggplot(genres, aes(music_genre, tempo, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average tempo', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")

p11 <- ggplot(genres, aes(music_genre, valence, fill = music_genre)) +     
  geom_bar(stat = "identity") +
   labs(title='Average valence', x = "Genre")+
  theme(axis.text = element_text(size = 3), axis.title = element_text(size = 5), plot.title = element_text(size = 10), legend.position="none") +
  scale_fill_brewer(palette = "PuOr")


p1 + p2 + p3 + p4 + p5 + p6 
p7 + p8 + p9 + p10 + p11
```



```{r, results = 'markup'}
Cont_Table <- table(music$music_genre, music$key)
xkabledply(Cont_Table, title="Contingency Table for Genre and Key")
chitest = chisq.test(Cont_Table)
chitest
```

```{r}
# drop those blank genre values 
#music = music[!(music$music_genre == ""),]

# convert tempo to numeric
#music$tempo <- as.numeric(music$tempo)

# created some na values...let's see what

#tempoNA <- music %>%
#     group_by(is.na(music$tempo)) %>%
#     count()
#tempoNA
# not enough to cause concern 
```
```{r}

# drop those blank genre values 
#music = music[!(music$music_genre == ""),]

# key, mode, genre as factor
#music$key <- factor(music$key)
#music$mode <- factor(music$mode)
#music$music_genre <- factor(music$music_genre)

# tempo as numeric
#music$tempo <- as.numeric(music$tempo)

# drop na values
#music = music[!(is.na(music$tempo)),]
```

Below, we test for statistically significant differences in our continuous variables using ANOVA tests and find that all show differences in means.

```{r, results = 'markup'}
# simplify dataset 
musicNum <- music[, c(4:9, 11:12, 14:15, 17, 18)]

# run tests
library(tidyverse)
anova_results <- purrr::map(musicNum[,1:11], ~summary(aov(.x ~ musicNum$music_genre)))
anova_results
```

# Section IV: Logit Modeling

We developed two types of Logit models to predict genre; multinomial logit regression and binomial logit regression. The multinomial model allows us to see what the second, third, etc. most likely genres are to get a better sense of where the model might be mis-classifying and why. Given the similarities between some genres, we expect that there would be some genres where this type of model might predict poorly. A binomial model won't allow us to see what the likeliest genres for a song might be, but because we can tailor each model to classify a particular genre, we may obtain better accuracy.

## Feature Selection

Before developing our model, we look to see which variables are correlated with each other to ensure we are not including variables that are too correlated. As shown below, we don’t want to include both `energy` and `acousticness`, or both `acousticness` and `loudness`, or `energy` and `loudness`. Since `energy` is correlated with `loudness` and `acousticness`, we leave it in the models to capture both those effects. 

```{r, results='markup'}
loadPkg("corrplot")
#music_cor <- music[, c(4:9, 11:12, 14:15, 17)]
corrplot(cor(music_cor, method = "spearman"), method = "circle", type = "upper")
```

```{r}
# prepare training and test data
set.seed(100)
trainingRows <- sample(1:nrow(music), 0.7*nrow(music))
training <- music[trainingRows, ]
test <- music[-trainingRows, ]
```

## Model I 

Our first multinomial Logit model includes the following variables:

* `popularity`
* `danceability`
* `duration_ms`
* `energy`
* `instrumentalness`
* `key`
* `liveness`
* `mode`
* `speechiness`
* `tempo`
* `valence`

All variables are statistically significant according to 2-tailed Wald tests at  the $\alpha$ = 0.05 level.

```{r, results = 'markup'}
# build model
multinomModel <- multinom(music_genre ~ popularity + danceability + duration_ms + energy + instrumentalness + key + liveness + mode + speechiness + tempo + valence, data=training) # multinom Model

model1 <- summary(multinomModel) 
model1

z <- model1$coefficients/model1$standard.errors
# 2-tailed Wald z tests to test significance of coefficients
```

Below are the p-values for the coefficients of the model based on Wald 2-sample tests. All p values are below 0.05. 

```{r}
p <- (1 - pnorm(abs(z), 0, 1)) * 2
xkabledply(p, title = "P-Values for Coefficients in Model 1")
```

```{r}
data1 <- list()
data2 <-list()
#cols <- c("Alt", "Alt-Rock", "Anime", "Blues", "Blues-Jazz", "Class", "Country", "Elec", "Hip-Hop", "HH-Rap", "Jazz", "Jazz-Blues", "Rap", "Rap-HH", "Rock", "Rock-Alt")
```

### Model I Performance 

```{r, results = 'markup'}
# see how model1 does

predicted_class <- predict (multinomModel, test)
table <- table(predicted_class, test$music_genre)
xkabledply(table, title = "Model 1 Confusion Matrix")


# Alt: .38
data1 <- c(data1, table[1,1]/sum(table[1,]))

# Alt or Rock? .51
# data1 <- c(data1, (table[1,1] + table[1,10])/sum(table[1,]))

# Anime: .60
data1 <- c(data1, table[2,2]/sum(table[2,]))

# Blues: .49
data1 <- c(data1, table[3,3]/sum(table[3,]))

# Blues or Jazz? .61
# data1 <- c(data1, (table[3,3] + table[3,8])/sum(table[3,]))

# Class: .70
data1 <- c(data1, table[4,4]/sum(table[4,]))

# Country: .44
data1 <- c(data1, table[5,5]/sum(table[5,]))

# Elec: .57
data1 <- c(data1, table[6,6]/sum(table[6,]))

# Hip-Hop: .44
data1 <- c(data1, table[7,7]/sum(table[7,]))

# Hip Hop or rap? .79
# data1 <- c(data1, (table[7,7] + table[7,9])/sum(table[7,]))

# Jazz: .50
data1 <- c(data1, table[8,8]/sum(table[8,]))

# Jazz or Blues? .62
# data1 <- c(data1, (table[8,8] + table[8,3])/sum(table[8,]))

# Rap: .43
data1 <- c(data1, table[9,9] /sum(table[9,]))

# Rap or Hip Hop? .88
# data1 <- c(data1, (table[9,9] + table[9,7])/sum(table[9,]))

# Rock: .53
data1 <- c(data1, table[10,10] /sum(table[10,]))

# Rock or Alt? .65
# data1 <- c(data1, (table[10,10] + table[10,1])/sum(table[10,]))
```



Now, we look to see how the model performs. The confusion matrix is listed above, and the total accuracy is `r mean(as.character(predicted_class) == as.character(test$music_genre))`. So, on average across all genres, the model predicts the correct genre roughly 68% of the time. The most challenging genres to predict (SMART Question III) were Alternative (accuracy: `r c(table[1,1]/sum(table[1,]))`) and Rap (accuracy: `r (table[9,9] /sum(table[9,]))`. However, these genres are similar to Rock and Hip-Hop respectively. If we include those genres as "correct," accuracy jumps for Alternative to `r (table[1,1] + table[1,10])/sum(table[1,])` and Rap to `r (table[9,9] + table[9,7])/sum(table[9,])`. 

If we predicted genres at random, we could expect to be right 10% of time time, so this accuracy rate is an improvement over a null model. Next, we look to see if we can simplify the model without sacrificing accuracy. 

Based on the confusion matrix, we can see which genres are most similar to each other based on this model (SMART Question 2). For example, when a country song is not being predicted as country (the prediction with the highest frequency), the second most-likely prediction is Alternative. 

* **Alternative:** Rock
* **Anime:** Blues
* **Blues:** Anime, Jazz, Country
* **Classical:** Anime, Jazz
* **Country:** Alternative, Rock
* **Electronic:** Jazz
* **Hip Hop:** Rap
* **Jazz:** Electronic, Blues
* **Rap:** Hip Hop
* **Rock:** Alterantive, Country


Accuracy rates for each genre are listed below:

```{r, results = 'markup'}
gens <- c("Alternative", "Anime", "Blues", "Classical", "Country", "Electronic", "Hip-Hop", "Jazz", "Rap", "Rock")
model1_data = data.frame(unlist(data1), row.names = gens)
names(model1_data) = "Accuracy"
xkabledply(model1_data, title = "Accuracy")
```

Alternative and Rock, Blues and Jazz, and Hip Hop and Rap are similar to each other. This is based in part on anecdotal knowledge and in part on the results from the confusion matrix above (e.g. is Rock a common missclassification of Aleternative?). If we consider these genres with expected overlap and similarities as "successes" if predicted, accuracy rates change to the following: 

* Predicting Alternative (accept Alternative or Rock): `r (table[1,1] + table[1,10])/sum(table[1,])`
* Predicting Rock (accept Alternative or Rock): `r (table[10,10] + table[10,1])/sum(table[10,])`
* Predicting Jazz (accept Jazz or Blues): `r (table[8,8] + table[8,3])/sum(table[8,])`
* Predicting Blues (accept Jazz or Blues): `r (table[3,3] + table[3,8])/sum(table[3,])`
* Predicting Hip Hop (accept Hip Hop or Rap): `r (table[7,7] + table[7,9])/sum(table[7,])`
* Predicting Rap (accept Hip Hop or Rap): `r (table[9,9] + table[9,7])/sum(table[9,])`

## Model II

This next model includes the following variables (does not include `key` to try and simplify the model):

* `popularity`
* `danceability`
* `duration_ms`
* `energy`
* `instrumentalness`
* `liveness`
* `mode`
* `speechiness`
* `tempo`
* `valence`

All variables are statistically significant according to 2-tailed Wald tests at the $\alpha$ = 0.05 level.

```{r}
# build model2
multinomModel2 <- multinom(music_genre ~ popularity + danceability + duration_ms + energy + instrumentalness + liveness + mode + speechiness + tempo + valence, data=training) # multinom Model

model2 <- summary(multinomModel2) 
model2

z <- model2$coefficients/model2$standard.errors
# 2-tailed Wald z tests to test significance of coefficients
p <- (1 - pnorm(abs(z), 0, 1)) * 2
print("Wald Test P-Values: ")
p
```

### Model II Performance 


```{r, results = 'markup'}
# see how model2 does

predicted_class2 <- predict (multinomModel2, test)

# All
table <- table(predicted_class2, test$music_genre)
# mean(as.character(predicted_class2) != as.character(test$music_genre))
xkabledply(table, title = "Model 2 Confusion Matrix")

# Alt: .37
data2 <- c(data2, table[1,1]/sum(table[1,]))

# Alt or Rock? .52
# data2 <- c(data2, (table[1,1] + table[1,10])/sum(table[1,]))

# Anime: .61
data2 <- c(data2, table[2,2]/sum(table[2,]))

# Blues: .49
data2 <- c(data2, table[3,3]/sum(table[3,]))

# Blues or Jazz? .62
# data2 <- c(data2, (table[3,3] + table[3,8])/sum(table[3,]))

# Class: .71
data2 <- c(data2, table[4,4]/sum(table[4,]))

# Country: .43
data2 <- c(data2, table[5,5]/sum(table[5,]))

# Elec: .56
data2 <- c(data2, table[6,6]/sum(table[6,]))

# Hip-Hop: .46
data2 <- c(data2, table[7,7]/sum(table[7,]))

# Hip Hop or rap? .79
# data2 <- c(data2, (table[7,7] + table[7,9])/sum(table[7,]))

# Jazz: .52
data2 <- c(data2, table[8,8]/sum(table[8,]))

# Jazz or Blues? .63
# data2 <- c(data2, (table[8,8] + table[8,3])/sum(table[8,]))

# Rap: .42
data2 <- c(data2, table[9,9] /sum(table[9,]))

# Rap or Hip Hop? .87
# data2 <- c(data2, (table[9,9] + table[9,7])/sum(table[9,]))

# Rock: .53
data2 <- c(data2, table[10,10] /sum(table[10,]))

# Rock or Alt? .66
# data2 <- c(data2, (table[10,10] + table[10,1])/sum(table[10,]))
```

Again, we look to see how the model performs. The confusion matrix is listed above, and the total accuracy is `r mean(as.character(test$predicted_class2) == as.character(test$music_genre))`. So, on average across all genres, the model predicts the correct genre roughly 68% of the time, nearly the same as the previous model. Overall, all accuracy rates for all genres are roughly the same, and removing the dummy variable `key` has simplified the model without sacrificing accuracy. Still, accuracy for some genres (notably Alternative) could be improved, and we will look to create binomial logit models.

In terms of genre characteristics (SMART Question 1), both multinomial models tell us that compared to Alternative music:

* Hip Hop, Rap, and Rock are the most popular 
* Country, Electronic, Hip Hop, Jazz, and Rap are the most danceable
* Country and Anime songs are shorter
* Electronic and Anime songs have higher energy
* Electronic, Hip Hop, and Rap songs are not very happy 
* Jazz, Blues, and Classical songs tend to be slower 


## Individual Logit Models

To prep the data for individual models, we create subsets of the data that are balanced with an even mix of the genre in question we are looking to predict and a random sampling of all other genres. The Logit regression model summaries for each genre are presented below. Overall, we see most variables are statistically significant at the $\alpha$ = 0.05 level. 

Each model also tells us a bit more about each genre as compared to the average characteristics of all other genres (SMART Question 1). Note that 'strongest predictors' are determined by the magnitude of the coefficient of the variable. Because our variables of comparison are measured on relatively equal scales (0 to 1, continuous), we consider these apt comparisons. 

* **Alternative:** more popular, energetic, and likely to be in a minor key. *Strongest Predictors*: high energy, not danceable, not instrumental, not speechy
* **Anime:** more energetic and faster. *Strongest Predictors*: not danceable, energetic, not live, not speechy
* **Blues:** more likely to contain a wider variety of keys (including those less common to other genres), more likely to be in a minor key. *Strongest Predictors*: not danceable, not instrumental, not speechy, happy.
* **Classical:** less popular. *Strongest Predictors*: not danceable, energetic, instrumental.
* **Country:** Danceable and happy. *Strongest Predictors*: Not instrumental, not in minor keys. 
* **Electronic:** *Strongest Predictors/Characteristics*: Very danceable, very instrumental, very energetic, not happy.
* **Hip Hop:** More popular, more likley to be in a minor key. *Strongest Predictors*: Very danceable, not instrumental, very speechy, not happy.
* **Jazz:** Similar to the Blues, more likely to contain a wider variety of keys (including those less common to other genres) and also more likely to be in a minor key. *Strongest Predictors*: danceable, not energetic, instrumental, not speechy, happy.
* **Rap:** More popular, energetic. *Strongest Predictors*: danceable, not instrumental, speechy, not happy.
* **Rock:** More popular, less likley to be in minor keys. *Strongest Predictors*: not danceable, energetic, happy.  


```{r, results = 'markup'}
# Logit models by genre

# make list of genres 
gens <- c("Alternative", "Anime", "Blues", "Classical", "Country", "Electronic", "Hip-Hop", "Jazz", "Rap", "Rock")

# set seed
set.seed(100)

# create list 
data3 = list()

for (genre in gens) {
# create balanced dataset
  music$selgen <- ifelse(music$music_genre == genre, 1, 0)
  selectGenre <- music[music$selgen == 1, ]
  otherGenre <- music[music$selgen == 0, ]
  otherGenre_sample <- sample(1:nrow(otherGenre), nrow(selectGenre))
  otherGenre <- otherGenre[otherGenre_sample, ]
  modelData <- data.frame(rbind(selectGenre, otherGenre))

# split into test/train
  trainingRows <- sample(1:nrow(modelData), 0.7*nrow(modelData))
  training <- modelData[trainingRows, ]
  test <- modelData[-trainingRows, ]

# model
  Model <- glm(selgen ~ popularity + danceability + duration_ms + energy + instrumentalness + key + liveness + mode + speechiness + tempo + valence, data = modelData, family = "binomial")

  print(genre)
  print(summary(Model))
  music$predselgen <- round(predict(Model, newdata = music, type = "response"))
  table = table(music$selgen, music$predselgen)
  accuracy = (table[1,1] + table[2,2]) / nrow(music)
  data3 = c(data3, accuracy)
}

```

Below is a table comparing the accuracy of all the different logit models. The baseline accuracy score for the individual models is 0.5 (1 in 2 chance of randomly guessing correctly), as compared to 0.1 for the multinomial models. Depending on the application, it may be more useful to choose one model over another. While the accuracy scores are higher for each genre as a whole when individual models are used, information on what the next highest predicted genre would be is lost. When dealing with songs that are in cusp genres (like Alternative) or are otherwise different than their typical genre, individual models may be less useful than the multinomial model. 

```{r, results = 'markup'}
# make results of two model accuracies by genre into a df

df = data.frame(unlist(data1), unlist(data2), unlist(data3), row.names = gens)
names(df) = c("Model 1", "Model 2", "Genre Spec. Model")
xkabledply(df, title = "Model Accuracy Comparisons")
```

Now, how do predictions work? The most interesting predictions to look at are from the multinomial model, which tells us (if incorrect) what the predicted genre actually is.

In this sample, there are a few notable mistakes; the Anime songs are both classified as classical, the electronic song is classified as jazz (though, the repeated chords do have a jazz sound to them), and the operatic classic "La Donna è Mobile" was classified as country. "Under the Pressure" was classified as rap, though in a previous iteration (with a different random sample not shown) this song was correctly classified as rock. 

```{r, results = 'markup'}
test$predicted <- predicted_class2 <- predict (multinomModel2, test)
set.seed(369)
samples <- sample_n(test[, c(1, 2, 3, 18, 21)], 10)
xkabledply(samples, title = "Sample Predictions")
```
Interestingly, classical wasn't even the second closest match for "La Donna è Mobile," but country. 

```{r, results = 'markup'}
predict (multinomModel2, test[ test$instance_id == '38780', ], type = 'probs')
```


```{r}
# make list of genres 
gens <- "Classical"

# set seed
set.seed(100)

# create list 
data3 = list()

for (genre in gens) {
# create balanced dataset
  music$selgen <- ifelse(music$music_genre == genre, 1, 0)
  selectGenre <- music[music$selgen == 1, ]
  otherGenre <- music[music$selgen == 0, ]
  otherGenre_sample <- sample(1:nrow(otherGenre), nrow(selectGenre))
  otherGenre <- otherGenre[otherGenre_sample, ]
  modelData <- data.frame(rbind(selectGenre, otherGenre))

# split into test/train
  trainingRows <- sample(1:nrow(modelData), 0.7*nrow(modelData))
  training <- modelData[trainingRows, ]
  test <- modelData[-trainingRows, ]

# model
  Model_Class <- glm(selgen ~ popularity + danceability + duration_ms + energy + instrumentalness + key + liveness + mode + speechiness + tempo + valence, data = modelData, family = "binomial")

  music$predselgen <- round(predict(Model_Class, newdata = music, type = "response"))
  table = table(music$selgen, music$predselgen)
  accuracy = (table[1,1] + table[2,2]) / nrow(music)
  data3 = c(data3, accuracy)
}
```

```{r}
predict(Model_Class, test[ test$instance_id == '38780', ], type = 'response')
```

However, when using a model calibrated to predict classical or not, "La Donna è Mobile," is predicted to be classical--but barely, at a probability of `r predict (Model_Class, test[ test$instance_id == '38780', ], type = 'response')`. 

# Section V: Decision Trees
Since a few genres were not successfully predicted by the logit models, decision tree modeling was employed in efforts to improve these predictions. Listed below are decision trees for alternative, rock, blues, jazz, hip-hop and rap.
```{r}
library(partykit)
library(rpart)
loadPkg("rpart.plot")
loadPkg("rattle")
library(caret)
library(ISLR)
loadPkg("class")
loadPkg("dplyr")
```

```{r}
# drop those blank genre values 
music$mode = factor(music$mode)
music$key = factor(music$key)
music$tempo <- as.numeric(music$tempo)

music<-na.omit(music)
str(music)

```
```{r genres as binary categorical data}
#alternate
new_genres_alt<-c()
new_genres_alt<-ifelse (music$music_genre=="Alternative", "a","na")
music$new_genres_alt<-new_genres_alt
music$new_genres_alt = factor(music$new_genres_alt)

#rock
new_genres_rock<-c()
new_genres_rock<-ifelse (music$music_genre=="Rock", "r","nr")
music$new_genres_rock<-new_genres_rock
music$new_genres_rock = factor(music$new_genres_rock)

#blue
new_genres_blue<-c()
new_genres_blue<-ifelse (music$music_genre=="Blues", "b","nb")
music$new_genres_blue<-new_genres_blue
music$new_genres_blue = factor(music$new_genres_blue)

#jazz
new_genres_jazz<-c()
new_genres_jazz<-ifelse (music$music_genre=="Jazz", "j","nj")
music$new_genres_jazz<-new_genres_jazz
music$new_genres_jazz = factor(music$new_genres_jazz)

#hip-hop
new_genres_hiphop<-c()
new_genres_hiphop<-ifelse (music$music_genre=="Hip-Hop", "h","nh")
music$new_genres_hiphop<-new_genres_hiphop
music$new_genres_hiphop = factor(music$new_genres_hiphop)

#rap
new_genres_rap<-c()
new_genres_rap<-ifelse (music$music_genre=="Rap", "rap","nrap")
music$new_genres_rap<-new_genres_rap
music$new_genres_rap = factor(music$new_genres_rap)
```
## Alterative 

```{r balancing data}
#train/test for alternate 

alt<-subset(music, new_genres_alt == "a")
not_alt<-subset(music, new_genres_alt == "na")
new_alt<-rbind(alt, sample_n(not_alt,5000))

set.seed(100)
trainingRows_alt <- sample(1:nrow(new_alt), 0.7*nrow(new_alt))
training_alt <- new_alt[trainingRows_alt, ]
testing_alt <- new_alt[-trainingRows_alt, ]
```

```{r}
#decision tree for alternative
treefit_alt<-rpart(new_genres_alt ~ popularity + acousticness + danceability + energy + instrumentalness + key +liveness + loudness + mode + speechiness + valence, data=training_alt, method="class", control = list(maxdepth = 4))
fancyRpartPlot(treefit_alt)
cm_alt = confusionMatrix(predict(treefit_alt, type = "class"), reference = training_alt$new_genres_alt)
cm_alt
```
The accuracy of the alternative decision tree model is `r cm_alt$overall['Accuracy']`

## Rock
```{r balancing data for rock}

rock<-subset(music, new_genres_rock == "r")
not_rock<-subset(music, new_genres_rock == "nr")
new_rock<-rbind(rock, sample_n(not_rock,5000))

set.seed(100)
trainingRows_rock <- sample(1:nrow(new_rock), 0.7*nrow(new_rock))
training_rock <- new_rock[trainingRows_rock, ]
testing_rock <- new_rock[-trainingRows_rock, ]
```

```{r}
#decision tree for rock
treefit_rock<-rpart(new_genres_rock ~ popularity + acousticness + danceability + energy + instrumentalness + key +liveness + loudness + mode + speechiness + valence, data=training_rock, method="class", control = list(maxdepth = 4))
fancyRpartPlot(treefit_rock)
cm_rock = confusionMatrix(predict(treefit_rock, type = "class"), reference = training_rock$new_genres_rock)
cm_rock
```
The accuracy of the rock decision tree model is `r cm_rock$overall['Accuracy']`


## Blues
```{r balancing data for blues}
blue<-subset(music, new_genres_blue == "b")
not_blue<-subset(music, new_genres_blue == "nb")
new_blue<-rbind(blue, sample_n(not_blue,5000))

set.seed(100)
trainingRows_blue <- sample(1:nrow(new_blue), 0.7*nrow(new_blue))
training_blue <- new_blue[trainingRows_blue, ]
testing_blue <- new_blue[-trainingRows_blue, ]

```

```{r decision tree for blues}
treefit_blue<-rpart(new_genres_blue ~ popularity + acousticness + danceability + energy + instrumentalness + key +liveness + loudness + mode + speechiness + valence, data=training_blue, method="class", control = list(maxdepth = 4))
fancyRpartPlot(treefit_blue)
cm_blue = confusionMatrix(predict(treefit_blue, type = "class"), reference = training_blue$new_genres_blue)
cm_blue
```
The accuracy of the blues decision tree model is `r cm_blue$overall['Accuracy']`


## Jazz
```{r balancing data for Jazz}
jazz<-subset(music, new_genres_jazz == "j")
not_jazz<-subset(music, new_genres_jazz == "nj")
new_jazz<-rbind(jazz, sample_n(not_jazz,5000))

set.seed(100)
trainingRows_jazz <- sample(1:nrow(new_jazz), 0.7*nrow(new_jazz))
training_jazz <- new_jazz[trainingRows_jazz, ]
testing_jazz <- new_jazz[-trainingRows_jazz, ]

```

```{r decision tree for jazz}
treefit_jazz<-rpart(new_genres_jazz ~ popularity + acousticness + danceability + energy + instrumentalness + key +liveness + loudness + mode + speechiness + valence, data=training_jazz, method="class", control = list(maxdepth = 4))
fancyRpartPlot(treefit_jazz)
cm_jazz = confusionMatrix(predict(treefit_jazz, type = "class"), reference = training_jazz$new_genres_jazz)
cm_jazz
```
The accuracy of the jazz decision tree model is `r cm_jazz$overall['Accuracy']`

## Hip-Hop
```{r balancing data for Hip-Hop}
hiphop<-subset(music, new_genres_hiphop == "h")
not_hiphop<-subset(music, new_genres_hiphop == "nh")
new_hiphop<-rbind(hiphop, sample_n(not_hiphop,5000))

set.seed(100)
trainingRows_hiphop <- sample(1:nrow(new_hiphop), 0.7*nrow(new_hiphop))
training_hiphop <- new_hiphop[trainingRows_hiphop, ]
testing_hiphop <- new_hiphop[-trainingRows_hiphop, ]

```

```{r decision tree for Hip-Hop}
treefit_hiphop<-rpart(new_genres_hiphop ~ popularity + acousticness + danceability + energy + instrumentalness+ key +liveness + loudness + mode + speechiness + tempo + valence, data=training_hiphop, method="class", control = list(maxdepth = 4))
fancyRpartPlot(treefit_hiphop)
cm_hiphop = confusionMatrix(predict(treefit_hiphop, type = "class"), reference = training_hiphop$new_genres_hiphop)
cm_hiphop
```
The accuracy of the hip-hop decision tree model is `r cm_hiphop$overall['Accuracy']`


## Rap
```{r balancing data for Rap}
rap<-subset(music, new_genres_rap == "rap")
not_rap<-subset(music, new_genres_rap == "nrap")
new_rap<-rbind(rap, sample_n(not_rap,5000))

set.seed(100)
trainingRows_rap <- sample(1:nrow(new_rap), 0.7*nrow(new_rap))
training_rap <- new_rap[trainingRows_rap, ]
testing_rap <- new_rap[-trainingRows_rap, ]

```

```{r decision tree for rap}
treefit_rap<-rpart(new_genres_rap ~ popularity + acousticness + danceability + energy + instrumentalness + key +liveness + loudness + mode + speechiness + valence, data=training_rap, method="class", control = list(maxdepth = 4))
fancyRpartPlot(treefit_rap)
printcp(treefit_rap)
cm_rap = confusionMatrix(predict(treefit_rap, type = "class"), reference = training_rap$new_genres_rap)
cm_rap
```
The accuracy of the rap decision tree model is `r cm_rap$overall['Accuracy']`


## Decision Tree Model Findings
Among the all the decision trees, hip-hop has the best accuracy and alternative has the worst accuracy. The hip-hop confusion matrix shows that the model incorrectly predicted 690 times as Hip-Hop and incorrectly predicted 215 times as not hip-hop. The alternative confusion matrix shows that the model incorrectly predicted 1097 times as alternative and incorrectly predicted 616 time as not alternative. Although all of the variables were in included in the decision tree formula, the algorithm only used the most important ones. So, the most important variables for the hip-hop decision tree are acousticness, danceability, energy, popularity, and speechiness.


# Section VI: KNN
Knn models were also used to see if they can better predict the six genres that had under performing logit models. The Knn models were developed using the variables that were found as most significant in the logit models. 


## Alterative 
```{r,results = 'markup'}
knn_alt=knn(train = training_alt[,c("energy", "danceability","instrumentalness", "speechiness")], test = testing_alt[,c("energy", "danceability","instrumentalness", "speechiness")], cl=training_alt[,"new_genres_alt"], k=100)
cm_knn_alt = confusionMatrix(knn_alt, reference = testing_alt[,"new_genres_alt"] )
cm_knn_alt
```
The accuracy of the alternative knn model is `r cm_knn_alt$overall['Accuracy']`


## Rock
```{r, results = 'markup'}
#knn for rock
knn_rock=knn(train = training_rock[,c("popularity", "danceability", "energy", "instrumentalness", "speechiness", "valence")], test = testing_rock[,c("popularity", "danceability", "energy", "instrumentalness", "speechiness", "valence")], cl=training_rock[,"new_genres_rock"], k=100)
cm_knn_rock = confusionMatrix(knn_rock, reference = testing_rock[,"new_genres_rock"] )
cm_knn_rock
```
The accuracy of the rock knn model is `r cm_knn_rock$overall['Accuracy']`

## Blues
```{r, results = 'markup'}
knn_blue=knn(train = training_blue[,c("popularity", "danceability", "energy", "instrumentalness","liveness", "speechiness","tempo", "valence")], test = testing_blue[,c("popularity", "danceability", "energy", "instrumentalness", "liveness", "speechiness", "tempo","valence")], cl=training_blue[,"new_genres_blue"], k=100)
cm_knn_blue = confusionMatrix(knn_blue, reference = testing_blue[,"new_genres_blue"] )
cm_knn_blue
```
The accuracy of the blues knn model is `r cm_knn_blue$overall['Accuracy']`


## Jazz
```{r , results = 'markup'}
knn_jazz=knn(train = training_jazz[,c("popularity", "danceability", "energy", "instrumentalness", "speechiness","tempo", "valence")], test = testing_jazz[,c("popularity", "danceability", "energy", "instrumentalness", "speechiness", "tempo","valence")], cl=training_jazz[,"new_genres_jazz"], k=100)
cm_knn_jazz = confusionMatrix(knn_jazz, reference = testing_jazz[,"new_genres_jazz"] )
cm_knn_jazz
```
The accuracy of the jazz knn model is `r cm_knn_jazz$overall['Accuracy']`


## Hip-Hop
```{r, results = 'markup'}
knn_hiphop=knn(train = training_hiphop[,c("popularity", "danceability", "energy", "instrumentalness","liveness", "speechiness", "valence")], test = testing_hiphop[,c("popularity", "danceability", "energy", "instrumentalness", "liveness", "speechiness", "valence")], cl=training_hiphop[,"new_genres_hiphop"], k=100)
cm_knn_hiphop = confusionMatrix(knn_hiphop, reference = testing_hiphop[,"new_genres_hiphop"] )
cm_knn_hiphop
```
The accuracy of the hip-hop knn model is `r cm_knn_hiphop$overall['Accuracy']`


## Rap
```{r , results = 'markup'}
knn_rap=knn(train = training_rap[,c("popularity", "danceability", "instrumentalness","liveness", "speechiness", "valence")], test = testing_rap[,c("popularity", "danceability", "instrumentalness", "liveness", "speechiness", "valence")], cl=training_rap[,"new_genres_rap"], k=100)
cm_knn_rap = confusionMatrix(knn_rap, reference = testing_rap[,"new_genres_rap"] )
cm_knn_rap
```
The accuracy of the rap knn model is `r cm_knn_rap$overall['Accuracy']`

## KNN Model Findings
Overall, all the knn models had lower accuracy score than the decision tree models. The rap model has the best accuracy and alternative has the worst accuracy. The rap confusion matrix shows that the model incorrectly predicted 65 times as the rap genre and incorrectly predicted 487 times as not rap. The alternative confusion matrix shows that the model incorrectly predicted 520 time as alternative and 509 times as not alternative. 


# Section VII: Conclusion

## Overall Results 

In sum, we found that the best predictors of a specific musical genre in terms of accuracy alone were our genre-specific models. Our binomial logit models and KNN models performed nearly identically for the selected genres, and our decision tree models provided some slight improvements over the logit and KNN models for most genres. Notably, the decision tree models for Alternative music and Jazz provide substantial improvement. 

However, while accuracy with these models is improved, they do not tell us much about why misclassification is occurring or where the predicted next best fits for a model are. Depending on the application of the models, a multinomial model may be a better fit. For example, if the goal is to use music features to automatically create playlists for each singular genre, our binary choice models are likely the better solution. However, for an artist curious to see where their style of music falls on more of spectrum, or an individual looking for music in a genre that's not what they typically listen too, but similar to their preferred genre, the multinomial model may be a better fit, even if accuracy is diminished.

## Lessons Learned and Model Improvement 

A next step to improve our models with the data we have readily available could be text analysis of artist names and/or song titles. Anecdotally, we can look at the subsample of our predicted data to see how text analysis would improve our model. For example, "追憶 - ゆきわりそう" is in Japanese; we may (or may not) want to associate songs with titles in Japanese more heavily with the Anime genre. Additionally, many classical songs contain numbers in the title (Act 3, Sonata No. 4). Looking at the title, one could guess that "La Donna é Mobile" would be classical rather than country. 

We could also consider adding interaction variables to adjust the weight given to certain factors. For example, "Under the Pressure" may be at risk of misclassification as rap because the words of the song are primarily spoken. However, it is also highly instrumental, which rap is not. By interacting these two variables, perhaps we would give less weight to speechiness (a strong predictor of rap) when the track is instrumental ('not instrumental' being a strong predictor of rap) and this would lead to tracks like this one not being misclassified. This could be done for other genres with a strong positive predictor and strong negative predictor. 

```{r, results = 'markup'}
xkabledply(samples, title = "Sample Predictions")
```

# References

Fabbri, Franco. (1981). A Theory of Musical Genres: Two Applications. Retrieved from https://www.tagg.org/xpdfs/ffabbri81a.pdf. 

Spotify. (2022). Spotify Web API (Reference). Retrieved from https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features. 